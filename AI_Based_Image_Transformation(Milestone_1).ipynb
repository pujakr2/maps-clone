{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pujakr2/maps-clone/blob/main/AI_Based_Image_Transformation(Milestone_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IUu74MWX-a-"
      },
      "source": [
        "#Task1: Convert original image into sketch image using opencv\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x3mTB9MXvTk",
        "outputId": "88df9c81-cd15-419c-d1e1-3db1dae23bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jcDTII6aaSLq"
      },
      "outputs": [],
      "source": [
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iAcvlMVKaYZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "e32aa15b-f3ee-49d5-ee63-f95ba15c23eb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.12.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3049494004.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Resize (optional — just to make it easier to view)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.12.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "# Load your image (replace 'your_image.jpg' with your file name)\n",
        "image = cv2.imread('babashaheb.jpg')\n",
        "\n",
        "# Resize (optional — just to make it easier to view)\n",
        "image = cv2.resize(image, (300, 300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pqHHb1-aotG"
      },
      "outputs": [],
      "source": [
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUo_D3nxa4of"
      },
      "outputs": [],
      "source": [
        "inverted_image = cv2.bitwise_not(gray_image)\n",
        "blurred = cv2.GaussianBlur(inverted_image, (21, 21), 0)\n",
        "inverted_blur = cv2.bitwise_not(blurred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzmGWKXya-J9"
      },
      "outputs": [],
      "source": [
        "pencil_sketch = cv2.divide(gray_image, inverted_blur, scale=256.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnK6wkAZbFFo"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-0a4wHobUMT"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(pencil_sketch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLH5x4ZFdmtG"
      },
      "source": [
        "Task2: Convert original image into sketch image without using opencv\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuemozljrWyU"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zXpUy3mrZOC"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()  # Choose an image from your device\n",
        "image_path = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSUEpV1LrqLe"
      },
      "outputs": [],
      "source": [
        "image = Image.open(image_path).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_EMzT4CrvNW"
      },
      "outputs": [],
      "source": [
        "gray = ImageOps.grayscale(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n29F5_2r2kI"
      },
      "outputs": [],
      "source": [
        "inverted = ImageOps.invert(gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcojQtUYr7OT"
      },
      "outputs": [],
      "source": [
        "blurred = inverted.filter(ImageFilter.GaussianBlur(radius=12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv0OzTdHsAOI"
      },
      "outputs": [],
      "source": [
        "gray_array = np.array(gray, dtype=float)\n",
        "blur_array = np.array(blurred, dtype=float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGHdIIRdsGHu"
      },
      "outputs": [],
      "source": [
        "pencil_sketch = 255 * (gray_array / (255 - blur_array + 1e-6))\n",
        "pencil_sketch = np.clip(pencil_sketch, 0, 255).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArkDEmipsPZm"
      },
      "outputs": [],
      "source": [
        "\n",
        "sketch_image = Image.fromarray(pencil_sketch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIm__OLRsQ5V"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Pencil Sketch\")\n",
        "plt.imshow(sketch_image, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOZ0Gb2F9Xp6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install opencv-python-headless pillow matplotlib numpy torch torchvision diffusers transformers accelerate safetensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8aMigY8_DiX"
      },
      "source": [
        "Task3: Convert an image into different styles of oil painting\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rbOOlWvA2Y0"
      },
      "source": [
        "1️⃣ OpenCV — Fast Stylization Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTCCBG0kA5mC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uki0C2XyBYHY"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLrZ8MBpBkK_"
      },
      "outputs": [],
      "source": [
        "img_bgr = cv2.imread(img_path)\n",
        "img_bgr = cv2.resize(img_bgr, (768, int(768 * img_bgr.shape[0] / img_bgr.shape[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYpKz7TVBs5E"
      },
      "outputs": [],
      "source": [
        "stylized = cv2.stylization(img_bgr, sigma_s=160, sigma_r=0.99)  # sigma_s: range 0-200, sigma_r: 0-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUYksQRBBt68"
      },
      "outputs": [],
      "source": [
        "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "stylized_rgb = cv2.cvtColor(stylized, cv2.COLOR_BGR2RGB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha82SQ91HEfD"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1); plt.title(\"Original\"); plt.imshow(img_rgb); plt.axis('off')\n",
        "plt.subplot(1,2,2); plt.title(\"OpenCV Stylized (oil-like)\"); plt.imshow(stylized_rgb); plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLnlrl2uDs2B"
      },
      "source": [
        "2️⃣ Stable Diffusion img2img — Multiple Oil Painting Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnOn2s__JffR"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI3v8FppJqSJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")  # set in runtime env or replace with your token string\n",
        "if HF_TOKEN == \"\":\n",
        "    print(\"Please set HF_TOKEN in environment before running. In Colab: %env HF_TOKEN=your_token_here\")\n",
        "else:\n",
        "    print(\"HF token present\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt6eVeFjJvZd"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "init_image = Image.open(img_path).convert(\"RGB\").resize((768, 512))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7xUoD3PJ2GY"
      },
      "outputs": [],
      "source": [
        "model_id = \"runwayml/stable-diffusion-v1-5\"  # or another SD1.5/2.1 model you have access to\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_safetensors=True, use_auth_token=HF_TOKEN)\n",
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzQXmLuSLOH7"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Oil painting, high detail, impasto brush strokes, dramatic lighting, by Ivan Aivazovsky, realistic\",\n",
        "    \"Classic oil portrait, Rembrandt style, warm tones, fine brushwork\",\n",
        "    \"Impressionist oil painting, Monet style, visible brushstrokes and soft edges, vibrant colors\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "strength = 0.7  # how much to change the original (0.4-0.8 typical)\n",
        "num_inference_steps = 30\n",
        "guidance_scale = 7.5\n",
        "\n",
        "for prompt in prompts:\n",
        "    out = pipe(prompt=prompt, image=init_image, strength=strength, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps)\n",
        "    results.append(out.images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9--p_7lWLdlQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,6))\n",
        "plt.subplot(1, len(results)+1, 1); plt.title(\"Original\"); plt.imshow(init_image); plt.axis('off')\n",
        "for i, img in enumerate(results):\n",
        "    plt.subplot(1, len(results)+1, i+2); plt.title(f\"Style {i+1}\"); plt.imshow(img); plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT1gkwkiLnsH"
      },
      "outputs": [],
      "source": [
        "for i, img in enumerate(results):\n",
        "    img.save(f\"sd_oil_style_{i+1}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqYfTQOCLvKx"
      },
      "source": [
        "3️⃣Neural Style Transfer (PyTorch VGG19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c6bzhhcPH5B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnTjV5hdPQMx"
      },
      "outputs": [],
      "source": [
        "print(\"Upload content (photo) then style (oil painting sample)\")\n",
        "\n",
        "# Upload content image\n",
        "print(\"Upload content image:\")\n",
        "uploaded_content = files.upload()\n",
        "if not uploaded_content:\n",
        "    print(\"No content file uploaded. Please upload a content image.\")\n",
        "else:\n",
        "    content_path = list(uploaded_content.keys())[0]\n",
        "    print(f\"Content image uploaded: {content_path}\")\n",
        "\n",
        "    # Upload style images one by one\n",
        "    style_paths = []\n",
        "    num_styles = 2  # You can change this to upload more style images\n",
        "    for i in range(num_styles):\n",
        "        print(f\"Upload style image {i+1}:\")\n",
        "        uploaded_style = files.upload()\n",
        "        if not uploaded_style:\n",
        "            print(f\"No file uploaded for style {i+1}. Skipping.\")\n",
        "            continue\n",
        "        style_paths.append(list(uploaded_style.keys())[0])\n",
        "        print(f\"Style image {i+1} uploaded: {style_paths[-1]}\")\n",
        "\n",
        "    # Now style_paths contains the paths to the uploaded style images\n",
        "    # The rest of the code in the following cells will use content_path and style_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iag1rrWbRI0Q"
      },
      "outputs": [],
      "source": [
        "def load_image(path, max_size=512):\n",
        "    image = Image.open(path).convert('RGB')\n",
        "    size = max(image.size)\n",
        "    if size > max_size:\n",
        "        scale = max_size / float(size)\n",
        "        image = image.resize((int(image.width*scale), int(image.height*scale)), Image.LANCZOS)\n",
        "    loader = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    return loader(image).unsqueeze(0).to(device)\n",
        "\n",
        "content = load_image(content_path, max_size=512)\n",
        "\n",
        "# Assuming content_path and style_paths are defined in the previous cell\n",
        "\n",
        "# The rest of the NST code should process each style image in style_paths\n",
        "# This will be handled in the next cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woGUlBMZRbE_"
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkwVtKbEReJI"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    b, c, h, w = tensor.size()\n",
        "    features = tensor.view(b * c, h * w)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(b * c * h * w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSuEg8X_Rlbf"
      },
      "outputs": [],
      "source": [
        "content_layers = ['21']  # relu4_2\n",
        "style_layers = ['0','5','10','19','28']  # relu1_1,relu2_1,relu3_1,relu4_1,relu5_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyrI5DGiRrPX"
      },
      "outputs": [],
      "source": [
        "def get_features(x):\n",
        "    features = {}\n",
        "    layers={'0':'conv1_1','5':'conv2_1','10':'conv3_1','19':'conv4_1','21':'conv4_2','28':'conv5_1'} # Corrected layer names based on VGG19 features\n",
        "    for name, layer in cnn._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "    return features\n",
        "\n",
        "content_features = get_features(content)\n",
        "content_layer = 'conv4_2' # Content layer\n",
        "content_weight = 1 # Weight for content loss\n",
        "style_weight = 1e6 # Weight for style loss\n",
        "\n",
        "\n",
        "results_nst = []\n",
        "titles_nst = []\n",
        "\n",
        "# Loop through each style image path\n",
        "for style_path in style_paths:\n",
        "    print(f\"Applying style from: {style_path}\")\n",
        "    style = load_image(style_path, max_size=512) # Load the style image\n",
        "    style_features = get_features(style)\n",
        "    style_grams = {l: gram_matrix(style_features[l]) for l in style_features}\n",
        "\n",
        "    # Initialize the target image as a clone of the content image\n",
        "    target = content.clone().requires_grad_(True).to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam([target], lr=0.02)\n",
        "\n",
        "    # Training loop\n",
        "    num_steps = 300 # Number of optimization steps (can be adjusted)\n",
        "    for step in range(num_steps):\n",
        "        target_features = get_features(target)\n",
        "\n",
        "        # Content loss\n",
        "        content_loss = torch.mean((target_features[content_layer]-content_features[content_layer])**2)\n",
        "\n",
        "        # Style loss\n",
        "        style_loss = 0\n",
        "        for l in style_grams:\n",
        "            t_feat = target_features[l]\n",
        "            t_gram = gram_matrix(t_feat)\n",
        "            s_gram = style_grams[l]\n",
        "            style_loss += torch.mean((t_gram - s_gram)**2)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = content_weight*content_loss + style_weight*style_loss\n",
        "\n",
        "        # Backpropagate and update\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward(retain_graph=True) # Added retain_graph=True\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (step+1) % 100 == 0:\n",
        "            print(f'Step [{step+1}/{num_steps}], Loss: {total_loss.item():.4f}')\n",
        "\n",
        "    # Detach the target image from the graph, move to CPU, clamp values and convert to PIL Image\n",
        "    out_img = target.detach().cpu().squeeze(0).clamp(0,1)\n",
        "    img_pil = transforms.ToPILImage()(out_img)\n",
        "\n",
        "    results_nst.append(img_pil)\n",
        "    titles_nst.append(\"NST Style: \"+style_path.split('/')[-1]) # Use filename as title\n",
        "\n",
        "# Display results (This part will be in the next cell usually, but added here for a single run)\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.subplot(1, len(results_nst) + 1, 1)\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(transforms.ToPILImage()(content.cpu().squeeze(0))) # Display original content image\n",
        "plt.axis(\"off\")\n",
        "\n",
        "for i, img in enumerate(results_nst):\n",
        "    plt.subplot(1, len(results_nst) + 1, i + 2)\n",
        "    plt.title(titles_nst[i])\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9FqiplwSZBU"
      },
      "outputs": [],
      "source": [
        "target = content.clone().requires_grad_(True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOozGRPGShQd"
      },
      "outputs": [],
      "source": [
        "style_weight = 1e6\n",
        "content_weight = 1e0\n",
        "optimizer = optim.LBFGS([target], lr=1.0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMX53q0fLSs0MjKTIefP9j1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}